

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>特性 &mdash; LightGBM 中文文档  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="实验" href="Experiments.html" />
    <link rel="prev" title="Python 包的相关介绍" href="Python-Intro.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> LightGBM 中文文档
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">内容目录:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Installation-Guide.html">安装指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="Quick-Start.html">快速入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="Python-Intro.html">Python 快速入门</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">特性</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">速度和内存使用的优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">稀疏优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">准确率的优化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#leaf-wise-best-first">Leaf-wise (Best-first) 的决策树生长策略</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">类别特征值的最优分割</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id6">网络通信的优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id7">并行学习的优化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">特征并行</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id9">传统算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lightgbm">LightGBM 中的特征并行</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id10">数据并行</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id11">传统算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">LightGBM中的数据并行</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id13">投票并行</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gpu">GPU 支持</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id14">应用和度量</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id15">其他特性</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Experiments.html">实验</a></li>
<li class="toctree-l1"><a class="reference internal" href="Parameters.html">参数</a></li>
<li class="toctree-l1"><a class="reference internal" href="Parameters-Tuning.html">参数优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="Python-API.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="Parallel-Learning-Guide.html">并行学习指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="GPU-Tutorial.html">GPU 教程</a></li>
<li class="toctree-l1"><a class="reference internal" href="Advanced-Topics.html">进阶主题</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="Development-Guide.html">开发指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="project-contributors.html">项目贡献者</a></li>
<li class="toctree-l1"><a class="reference internal" href="apachecn-learning-group.html">组织学习交流群</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="GPU-Performance.html">GPU Tuning Guide and Performance Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="GPU-Targets.html">GPU SDK 相关以及设备对应表</a></li>
<li class="toctree-l1"><a class="reference internal" href="GPU-Windows.html">GPU Windows Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="gcc-Tips.html">Recommendations When Using gcc</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html">Documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">LightGBM 中文文档</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>特性</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/Features.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>特性<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>这篇文档是对 LightGBM 的特点和其中用到的算法的简短介绍</p>
<p>本页不包含详细的算法，如果你对这些算法感兴趣可以查阅引用的论文或者源代码</p>
<div class="section" id="id2">
<h2>速度和内存使用的优化<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>许多提升工具对于决策树的学习使用基于 pre-sorted 的算法 <a class="reference external" href="#references">[1, 2]</a> (例如，在xgboost中默认的算法) ，这是一个简单的解决方案，但是不易于优化。</p>
<p>LightGBM 利用基于 histogram 的算法 <a class="reference external" href="#references">[3, 4, 5]</a>，通过将连续特征（属性）值分段为 discrete bins 来加快训练的速度并减少内存的使用。
如下的是基于 histogram 算法的优点：</p>
<ul class="simple">
<li><strong>减少分割增益的计算量</strong><ul>
<li>Pre-sorted 算法需要 <code class="docutils literal notranslate"><span class="pre">O(#data)</span></code> 次的计算</li>
<li>Histogram 算法只需要计算 <code class="docutils literal notranslate"><span class="pre">O(#bins)</span></code> 次, 并且 <code class="docutils literal notranslate"><span class="pre">#bins</span></code> 远少于 <code class="docutils literal notranslate"><span class="pre">#data</span></code><ul>
<li>这个仍然需要 <code class="docutils literal notranslate"><span class="pre">O(#data)</span></code> 次来构建直方图, 而这仅仅包含总结操作</li>
</ul>
</li>
</ul>
</li>
<li><strong>通过直方图的相减来进行进一步的加速</strong><ul>
<li>在二叉树中可以通过利用叶节点的父节点和相邻节点的直方图的相减来获得该叶节点的直方图</li>
<li>所以仅仅需要为一个叶节点建立直方图 (其 <code class="docutils literal notranslate"><span class="pre">#data</span></code> 小于它的相邻节点)就可以通过直方图的相减来获得相邻节点的直方图，而这花费的代价(<code class="docutils literal notranslate"><span class="pre">O(#bins)</span></code>)很小。</li>
</ul>
</li>
<li><strong>减少内存的使用</strong><ul>
<li>可以将连续的值替换为 discrete bins。 如果 <code class="docutils literal notranslate"><span class="pre">#bins</span></code> 较小, 可以利用较小的数据类型来存储训练数据, 如 uint8_t。</li>
<li>无需为 pre-sorting 特征值存储额外的信息</li>
</ul>
</li>
<li><strong>减少并行学习的通信代价</strong></li>
</ul>
</div>
<div class="section" id="id3">
<h2>稀疏优化<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>对于稀疏的特征仅仅需要 <code class="docutils literal notranslate"><span class="pre">O(2</span> <span class="pre">*</span> <span class="pre">#non_zero_data)</span></code> 来建立直方图</li>
</ul>
</div>
<div class="section" id="id4">
<h2>准确率的优化<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<div class="section" id="leaf-wise-best-first">
<h3>Leaf-wise (Best-first) 的决策树生长策略<a class="headerlink" href="#leaf-wise-best-first" title="Permalink to this headline">¶</a></h3>
<p>大部分决策树的学习算法通过 level(depth)-wise 策略生长树，如下图一样：</p>
<img alt="_images/level-wise.png" class="align-center" src="_images/level-wise.png" />
<p>LightGBM 通过 leaf-wise (best-first)<a class="reference external" href="#references">[6]</a> 策略来生长树。它将选取具有最大 delta loss 的叶节点来生长。
当生长相同的 <code class="docutils literal notranslate"><span class="pre">#leaf</span></code>，leaf-wise 算法可以比 level-wise 算法减少更多的损失。</p>
<p>当 <code class="docutils literal notranslate"><span class="pre">#data</span></code> 较小的时候，leaf-wise 可能会造成过拟合。
所以，LightGBM 可以利用额外的参数 <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> 来限制树的深度并避免过拟合（树的生长仍然通过 leaf-wise 策略）。</p>
<img alt="_images/leaf-wise.png" class="align-center" src="_images/leaf-wise.png" />
</div>
<div class="section" id="id5">
<h3>类别特征值的最优分割<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>我们通常将类别特征转化为 one-hot coding。
然而，对于学习树来说这不是个好的解决方案。
原因是，对于一个基数较大的类别特征，学习树会生长的非常不平衡，并且需要非常深的深度才能来达到较好的准确率。</p>
<p>事实上，最好的解决方案是将类别特征划分为两个子集，总共有 <code class="docutils literal notranslate"><span class="pre">2^(k-1)</span> <span class="pre">-</span> <span class="pre">1</span></code> 种可能的划分
但是对于回归树 <a class="reference external" href="#references">[7]</a> 有个有效的解决方案。为了寻找最优的划分需要大约 <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">*</span> <span class="pre">log(k)</span></code> .</p>
<p>基本的思想是根据训练目标的相关性对类别进行重排序。
更具体的说，根据累加值(<code class="docutils literal notranslate"><span class="pre">sum_gradient</span> <span class="pre">/</span> <span class="pre">sum_hessian</span></code>)重新对（类别特征的）直方图进行排序，然后在排好序的直方图中寻找最好的分割点。</p>
</div>
</div>
<div class="section" id="id6">
<h2>网络通信的优化<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>LightGBM 中的并行学习，仅仅需要使用一些聚合通信算法，例如 “All reduce”, “All gather” 和 “Reduce scatter”.
LightGBM 实现了 state-of-art 算法 <a class="reference external" href="#references">[8]</a> .
这些聚合通信算法可以提供比点对点通信更好的性能。</p>
</div>
<div class="section" id="id7">
<h2>并行学习的优化<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>LightGBM 提供以下并行学习优化算法：</p>
<div class="section" id="id8">
<h3>特征并行<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id9">
<h4>传统算法<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<p>传统的特征并行算法旨在于在并行化决策树中的“ <code class="docutils literal notranslate"><span class="pre">Find</span> <span class="pre">Best</span> <span class="pre">Split</span></code>.主要流程如下:</p>
<ol class="arabic simple">
<li>垂直划分数据（不同的机器有不同的特征集）</li>
<li>在本地特征集寻找最佳划分点 {特征, 阈值}</li>
<li>本地进行各个划分的通信整合并得到最佳划分</li>
<li>以最佳划分方法对数据进行划分，并将数据划分结果传递给其他线程</li>
<li>其他线程对接受到的数据进一步划分</li>
</ol>
<p>传统的特征并行方法主要不足:</p>
<ul class="simple">
<li>存在计算上的局限，传统特征并行无法加速 “split”（时间复杂度为 “O（#data）”）。
因此，当数据量很大的时候，难以加速。</li>
<li>需要对划分的结果进行通信整合，其额外的时间复杂度约为 “O（#data/8）”（一个数据一个字节）</li>
</ul>
</div>
<div class="section" id="lightgbm">
<h4>LightGBM 中的特征并行<a class="headerlink" href="#lightgbm" title="Permalink to this headline">¶</a></h4>
<p>既然在数据量很大时，传统数据并行方法无法有效地加速，我们做了一些改变：不再垂直划分数据，即每个线程都持有全部数据。
因此，LighetGBM中没有数据划分结果之间通信的开销，各个线程都知道如何划分数据。
而且，“#data” 不会变得更大，所以，在使每天机器都持有全部数据是合理的。</p>
<p>LightGBM 中特征并行的流程如下：</p>
<ol class="arabic simple">
<li>每个线程都在本地数据集上寻找最佳划分点｛特征， 阈值｝</li>
<li>本地进行各个划分的通信整合并得到最佳划分</li>
<li>执行最佳划分</li>
</ol>
<p>然而，该特征并行算法在数据量很大时仍然存在计算上的局限。因此，建议在数据量很大时使用数据并行。</p>
</div>
</div>
<div class="section" id="id10">
<h3>数据并行<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id11">
<h4>传统算法<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<p>数据并行旨在于并行化整个决策学习过程。数据并行的主要流程如下：</p>
<ol class="arabic simple">
<li>水平划分数据</li>
<li>线程以本地数据构建本地直方图</li>
<li>将本地直方图整合成全局整合图</li>
<li>在全局直方图中寻找最佳划分，然后执行此划分</li>
</ol>
<p>传统数据划分的不足：</p>
<ul class="simple">
<li>高通讯开销。
如果使用点对点的通讯算法，一个机器的通讯开销大约为 “O(#machine * #feature * #bin)” 。
如果使用集成的通讯算法（例如， “All Reduce”等），通讯开销大约为 “O(2 * #feature * #bin)”[8] 。</li>
</ul>
</div>
<div class="section" id="id12">
<h4>LightGBM中的数据并行<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h4>
<p>LightGBM 中采用以下方法较少数据并行中的通讯开销：</p>
<ol class="arabic simple">
<li>不同于“整合所有本地直方图以形成全局直方图”的方式，LightGBM 使用分散规约(Reduce scatter)的方式对不同线程的不同特征（不重叠的）进行整合。
然后线程从本地整合直方图中寻找最佳划分并同步到全局的最佳划分中。</li>
<li>如上所述。LightGBM 通过直方图做差法加速训练。
基于此，我们可以进行单叶子的直方图通讯，并且在相邻直方图上使用做差法。</li>
</ol>
<p>通过上述方法，LightGBM 将数据并行中的通讯开销减少到 “O(0.5 * #feature * #bin)”。</p>
</div>
</div>
<div class="section" id="id13">
<h3>投票并行<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>投票并行未来将致力于将“数据并行”中的通讯开销减少至常数级别。
其将会通过两阶段的投票过程较少特征直方图的通讯开销 <a class="reference external" href="#references">[9]</a> .</p>
</div>
</div>
<div class="section" id="gpu">
<h2>GPU 支持<a class="headerlink" href="#gpu" title="Permalink to this headline">¶</a></h2>
<p>感谢 “&#64;huanzhang12 &lt;<a class="reference external" href="https://github.com/huanzhang12">https://github.com/huanzhang12</a>&gt;” 对此项特性的贡献。相关细节请阅读 <a class="reference external" href="#references">[10]</a> 。</p>
<ul class="simple">
<li><a class="reference external" href="./Installatn-ioGuide.rst#build-gpu-version">GPU 安装</a></li>
<li><a class="reference external" href="./GPU-Tutorial.rst">GPU 训练</a></li>
</ul>
</div>
<div class="section" id="id14">
<h2>应用和度量<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>支持以下应用:</p>
<ul class="simple">
<li>回归，目标函数为 L2 loss</li>
<li>二分类， 目标函数为 logloss（对数损失）</li>
<li>多分类</li>
<li>lambdarank, 目标函数为基于 NDCG 的 lambdarank</li>
</ul>
<p>支持的度量</p>
<ul class="simple">
<li>L1 loss</li>
<li>L2 loss</li>
<li>Log loss</li>
<li>Classification error rate</li>
<li>AUC</li>
<li>NDCG</li>
<li>Multi class log loss</li>
<li>Multi class error rate</li>
</ul>
<p>获取更多详情，请至 <a class="reference external" href="./Parameters.rst#metric-parameters">Parameters</a>。</p>
</div>
<div class="section" id="id15">
<h2>其他特性<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Limit <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> of tree while grows tree leaf-wise</li>
<li><a class="reference external" href="https://arxiv.org/abs/1505.01866">DART</a></li>
<li>L1/L2 regularization</li>
<li>Bagging</li>
<li>Column(feature) sub-sample</li>
<li>Continued train with input GBDT model</li>
<li>Continued train with the input score file</li>
<li>Weighted training</li>
<li>Validation metric output during training</li>
<li>Multi validation data</li>
<li>Multi metrics</li>
<li>Early stopping (both training and prediction)</li>
<li>Prediction for leaf index</li>
</ul>
<p>获取更多详情，请参阅 <a class="reference external" href="./Parameters.rst">参数</a>。</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>[1] Mehta, Manish, Rakesh Agrawal, and Jorma Rissanen. “SLIQ: A fast scalable classifier for data mining.” International Conference on Extending Database Technology. Springer Berlin Heidelberg, 1996.</p>
<p>[2] Shafer, John, Rakesh Agrawal, and Manish Mehta. “SPRINT: A scalable parallel classifier for data mining.” Proc. 1996 Int. Conf. Very Large Data Bases. 1996.</p>
<p>[3] Ranka, Sanjay, and V. Singh. “CLOUDS: A decision tree classifier for large datasets.” Proceedings of the 4th Knowledge Discovery and Data Mining Conference. 1998.</p>
<p>[4] Machado, F. P. “Communication and memory efficient parallel decision tree construction.” (2003).</p>
<p>[5] Li, Ping, Qiang Wu, and Christopher J. Burges. “Mcrank: Learning to rank using multiple classification and gradient boosting.” Advances in neural information processing systems. 2007.</p>
<p>[6] Shi, Haijian. “Best-first decision tree learning.” Diss. The University of Waikato, 2007.</p>
<p>[7] Walter D. Fisher. “<a class="reference external" href="http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479">On Grouping for Maximum Homogeneity</a>.” Journal of the American Statistical Association. Vol. 53, No. 284 (Dec., 1958), pp. 789-798.</p>
<p>[8] Thakur, Rajeev, Rolf Rabenseifner, and William Gropp. “<a class="reference external" href="http://wwwi10.lrr.in.tum.de/~gerndt/home/Teaching/HPCSeminar/mpich_multi_coll.pdf">Optimization of collective communication operations in MPICH</a>.” International Journal of High Performance Computing Applications 19.1 (2005): 49-66.</p>
<p>[9] Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tieyan Liu. “<a class="reference external" href="http://papers.nips.cc/paper/6381-a-communication-efficient-parallel-algorithm-for-decision-tree">A Communication-Efficient Parallel Algorithm for Decision Tree</a>.” Advances in Neural Information Processing Systems 29 (NIPS 2016).</p>
<p>[10] Huan Zhang, Si Si and Cho-Jui Hsieh. “<a class="reference external" href="https://arxiv.org/abs/1706.08359">GPU Acceleration for Large-scale Tree Boosting</a>.” arXiv:1706.08359, 2017.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Experiments.html" class="btn btn-neutral float-right" title="实验" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Python-Intro.html" class="btn btn-neutral" title="Python 包的相关介绍" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Microsoft Corporation. Translated by ApacheCN 开源组织, 机器学习企鹅交流群: 629470233

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/js/rst_links_fix.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-102475051-8');
</script>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b8d9c7f39d80628c950dbae02000d5ce";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        }
        else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
    </script>

<!-- ssi include stats -->
<!--#include virtual="/static/stats/global.shtml"-->
<!--#include virtual="/static/stats/custom.shtml"-->



</body>
</html>